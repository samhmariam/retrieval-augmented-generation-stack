{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a2e543d",
   "metadata": {},
   "source": [
    "#### The Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1fc63",
   "metadata": {},
   "source": [
    "#### 1.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f338245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aa3d41add84daea2713d90b742736d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  44%|####4     | 346M/784M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64a6150169a46dd826664473f8f2f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c1f3d980b14ff3bd8ff32756f7eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c88b50502e143f1afc2648c3fea757b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ca4c1190fc4a1a9f2caf54d21c9773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e69cbc5b8b46838b248d15ecf140cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name =  \"BAAI/bge-base-en-v1.5\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcf4b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\RAG\\retrieval-augmented-generation-stack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "res = model.encode(\"RAG is awesome\")\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b2d97",
   "metadata": {},
   "source": [
    "#### 1.2 Embeddings in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9398df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector, vector_array):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between a given vector and an array of vectors.\n",
    "    \n",
    "    Args:\n",
    "        vector (np.ndarray): A 1D numpy array representing the query vector\n",
    "        vector_array (np.ndarray): A 2D numpy array where each row is a vector\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of cosine similarity scores between the vector and each vector in the array\n",
    "    \"\"\"\n",
    "    # Normalize the query vector\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    if vector_norm == 0:\n",
    "        return np.zeros(len(vector_array))\n",
    "    \n",
    "    normalized_vector = vector / vector_norm\n",
    "    \n",
    "    # Normalize each vector in the array\n",
    "    vector_array_norms = np.linalg.norm(vector_array, axis=1)\n",
    "    # Handle zero vectors\n",
    "    zero_mask = vector_array_norms == 0\n",
    "    vector_array_norms[zero_mask] = 1  # Avoid division by zero\n",
    "    \n",
    "    normalized_vector_array = vector_array / vector_array_norms[:, np.newaxis]\n",
    "    \n",
    "    # Compute dot product (cosine similarity for normalized vectors)\n",
    "    similarities = np.dot(normalized_vector_array, normalized_vector)\n",
    "    \n",
    "    # Set similarity to 0 for zero vectors\n",
    "    similarities[zero_mask] = 0\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ab63d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5 embeddings with shape (768,)\n",
      "\n",
      "Cosine similarities:\n",
      "'RAG is awesome' vs 'Retrieval Augmented Generation is great': 0.5172\n",
      "'RAG is awesome' vs 'I love machine learning': 0.5403\n",
      "'RAG is awesome' vs 'The weather is nice today': 0.5209\n",
      "'RAG is awesome' vs 'Vector embeddings are useful': 0.4673\n",
      "\n",
      "Most similar sentence: 'I love machine learning' (similarity: 0.5403)\n"
     ]
    }
   ],
   "source": [
    "# Example usage of cosine similarity function\n",
    "sentences = [\n",
    "    \"RAG is awesome\",\n",
    "    \"Retrieval Augmented Generation is great\",\n",
    "    \"I love machine learning\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Vector embeddings are useful\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Generated {len(embeddings)} embeddings with shape {embeddings[0].shape}\")\n",
    "\n",
    "# Use the first sentence as our query\n",
    "query_embedding = embeddings[0]\n",
    "document_embeddings = embeddings[1:]  # All other sentences\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "\n",
    "print(\"\\nCosine similarities:\")\n",
    "for i, (sentence, similarity) in enumerate(zip(sentences[1:], similarities)):\n",
    "    print(f\"'{sentences[0]}' vs '{sentence}': {similarity:.4f}\")\n",
    "\n",
    "# Find the most similar sentence\n",
    "most_similar_idx = np.argmax(similarities)\n",
    "print(f\"\\nMost similar sentence: '{sentences[most_similar_idx + 1]}' (similarity: {similarities[most_similar_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf467a7",
   "metadata": {},
   "source": [
    "#### 1.3 Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f38e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfeb04e",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1592304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  category\n",
      "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...         7\n",
      "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...         4\n",
      "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...         4\n",
      "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...         1\n",
      "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...        14\n",
      "\n",
      "Dataset Size: (11314, 2)\n",
      "\n",
      "Number of Categories: 20\n",
      "\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "# Convert the dataset to a DataFrame for easier handling\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups_train.data,\n",
    "    'category': newsgroups_train.target\n",
    "})\n",
    "\n",
    "# Display some basic information about the dataset\n",
    "print(df.head())\n",
    "print(\"\\nDataset Size:\", df.shape)\n",
    "print(\"\\nNumber of Categories:\", len(newsgroups_train.target_names))\n",
    "print(\"\\nCategories:\", newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0784dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "\tFrom: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CATEGORY:\n",
      "\trec.autos\n"
     ]
    }
   ],
   "source": [
    "print(f\"TEXT:\\n\\t{df['text'][0]}\\nCATEGORY:\\n\\t{newsgroups_train.target_names[df['category'][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084fb0c",
   "metadata": {},
   "source": [
    "#### Preprocessing and Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2557ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text data...\n",
      "Original dataset size: 11314\n",
      "After preprocessing: 11307\n",
      "Removed 7 empty/short texts\n",
      "\n",
      "Example of cleaned text:\n",
      "Original:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out...\n",
      "\n",
      "Cleaned:\n",
      "i was wondering if anyone out there could enlighten me on this car i saw the other day. it was a 2-door sports car, looked to be from the late 60s/ early 70s. it was called a bricklin. the doors were ...\n",
      "Original dataset size: 11314\n",
      "After preprocessing: 11307\n",
      "Removed 7 empty/short texts\n",
      "\n",
      "Example of cleaned text:\n",
      "Original:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out...\n",
      "\n",
      "Cleaned:\n",
      "i was wondering if anyone out there could enlighten me on this car i saw the other day. it was a 2-door sports car, looked to be from the late 60s/ early 70s. it was called a bricklin. the doors were ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove email headers and metadata (common in newsgroups)\n",
    "    text = re.sub(r'^.*?subject:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'^.*?from:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'^.*?date:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'^.*?organization:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'^.*?lines:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'^.*?nntp-posting-host:.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove quoted text (lines starting with > or >>)\n",
    "    text = re.sub(r'^>.*$', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove very short texts (less than 10 characters)\n",
    "    if len(text) < 10:\n",
    "        return \"\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "print(\"Preprocessing text data...\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty texts after preprocessing\n",
    "df_clean = df[df['cleaned_text'] != ''].copy()\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"After preprocessing: {len(df_clean)}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} empty/short texts\")\n",
    "\n",
    "# Display a sample of cleaned text\n",
    "print(\"\\nExample of cleaned text:\")\n",
    "print(f\"Original:\\n{df['text'].iloc[0][:200]}...\")\n",
    "print(f\"\\nCleaned:\\n{df_clean['cleaned_text'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae83ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text data using sentence-transformers model...\n",
      "Processing 1000 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1051ed79f8d34a75a34fb09f7e8cb7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\RAG\\retrieval-augmented-generation-stack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated embeddings shape: (1000, 768)\n",
      "Each document is represented as a 768-dimensional vector\n",
      "\n",
      "Embedding statistics:\n",
      "Min value: -0.3456\n",
      "Max value: 0.1493\n",
      "Mean value: -0.0007\n",
      "Standard deviation: 0.0361\n",
      "\n",
      "Category distribution in subset:\n",
      "soc.religion.christian: 67 documents\n",
      "comp.sys.mac.hardware: 60 documents\n",
      "rec.sport.baseball: 56 documents\n",
      "comp.os.ms-windows.misc: 55 documents\n",
      "talk.politics.mideast: 55 documents\n",
      "misc.forsale: 55 documents\n",
      "rec.autos: 54 documents\n",
      "sci.electronics: 54 documents\n",
      "alt.atheism: 54 documents\n",
      "sci.med: 52 documents\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the cleaned text using the pre-trained sentence transformer model\n",
    "print(\"Vectorizing text data using sentence-transformers model...\")\n",
    "\n",
    "# For demonstration, let's use a subset of the data to avoid long processing times\n",
    "# You can increase this number or remove the limit for full dataset processing\n",
    "max_samples = 1000\n",
    "df_subset = df_clean.head(max_samples).copy()\n",
    "\n",
    "print(f\"Processing {len(df_subset)} documents...\")\n",
    "\n",
    "# Generate embeddings for the cleaned text\n",
    "# Using show_progress_bar=True to display progress\n",
    "text_embeddings = model.encode(\n",
    "    df_subset['cleaned_text'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=False,\n",
    "    normalize_embeddings=True  # Normalize for better similarity computation\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Each document is represented as a {text_embeddings.shape[1]}-dimensional vector\")\n",
    "\n",
    "# Add embeddings to the dataframe for easier access\n",
    "df_subset['embeddings'] = list(text_embeddings)\n",
    "\n",
    "# Display some statistics\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"Min value: {text_embeddings.min():.4f}\")\n",
    "print(f\"Max value: {text_embeddings.max():.4f}\")\n",
    "print(f\"Mean value: {text_embeddings.mean():.4f}\")\n",
    "print(f\"Standard deviation: {text_embeddings.std():.4f}\")\n",
    "\n",
    "# Show categories distribution in our subset\n",
    "category_counts = df_subset['category'].value_counts()\n",
    "print(f\"\\nCategory distribution in subset:\")\n",
    "for cat_id, count in category_counts.head(10).items():\n",
    "    print(f\"{newsgroups_train.target_names[cat_id]}: {count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ff7b5",
   "metadata": {},
   "source": [
    "#### Function for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "372d449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for: 'machine learning and artificial intelligence'\n",
      "==================================================\n",
      "\n",
      "1. Similarity: 0.6669\n",
      "   Category: comp.graphics\n",
      "   Text: originator: keywords: conference reply-to: (ed breen) australian pattern recognition society 2nd call for papers dicta-93 2nd conference on - digital imaging computing: techniques and applications loc...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Similarity: 0.5941\n",
      "   Category: talk.politics.misc\n",
      "   Text: in-reply-to: message of sun, 4 apr 93 23:02:33 gmt reply-to: *teddy o'neill-creature with furry hobbit feet from bath uk*, a sentimental fool, posts: with the force of a world-wide youth movement, it ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Similarity: 0.5937\n",
      "   Category: sci.crypt\n",
      "   Text: in-reply-to: message of 20 apr 1993 16: 47:03 gmt mark riordan writes: [a list of large-integer arithmetic packages elided] i thought i would note that except lenstra's packages, none of the large-int...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\Downloads\\RAG\\retrieval-augmented-generation-stack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate retrieval using the vectorized data\n",
    "def search_similar_documents(query_text, embeddings_df, top_k=5):\n",
    "    \"\"\"\n",
    "    Search for similar documents using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): Query text to search for\n",
    "        embeddings_df (pd.DataFrame): DataFrame containing embeddings\n",
    "        top_k (int): Number of top similar documents to return\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Top similar documents with similarity scores\n",
    "    \"\"\"\n",
    "    # Preprocess and encode the query\n",
    "    cleaned_query = preprocess_text(query_text)\n",
    "    query_embedding = model.encode([cleaned_query], normalize_embeddings=True)[0]\n",
    "    \n",
    "    # Compute similarities with all documents\n",
    "    document_embeddings = np.array(embeddings_df['embeddings'].tolist())\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "    \n",
    "    # Get top-k most similar documents\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    results = embeddings_df.iloc[top_indices].copy()\n",
    "    results['similarity_score'] = similarities[top_indices]\n",
    "    \n",
    "    return results[['cleaned_text', 'category', 'similarity_score']]\n",
    "\n",
    "# Example search\n",
    "query = \"machine learning and artificial intelligence\"\n",
    "print(f\"Searching for: '{query}'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "similar_docs = search_similar_documents(query, df_subset, top_k=3)\n",
    "\n",
    "for idx, (_, row) in enumerate(similar_docs.iterrows(), 1):\n",
    "    category_name = newsgroups_train.target_names[row['category']]\n",
    "    print(f\"\\n{idx}. Similarity: {row['similarity_score']:.4f}\")\n",
    "    print(f\"   Category: {category_name}\")\n",
    "    print(f\"   Text: {row['cleaned_text'][:200]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976832e",
   "metadata": {},
   "source": [
    "#### Retrieving metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd06c9",
   "metadata": {},
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_precision_recall(query_text, expected_category, embeddings_df, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute precision and recall for a single query.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): Query text\n",
    "        expected_category (int): Expected category ID for relevant documents\n",
    "        embeddings_df (pd.DataFrame): DataFrame containing embeddings\n",
    "        top_k (int): Number of top documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (precision, recall, retrieved_docs)\n",
    "    \"\"\"\n",
    "    # Get search results\n",
    "    results = search_similar_documents(query_text, embeddings_df, top_k=top_k)\n",
    "    \n",
    "    # Calculate relevant documents in the retrieved set\n",
    "    relevant_retrieved = sum(results['category'] == expected_category)\n",
    "    \n",
    "    # Calculate total relevant documents in the dataset\n",
    "    total_relevant = sum(embeddings_df['category'] == expected_category)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = relevant_retrieved / top_k if top_k > 0 else 0\n",
    "    recall = relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "    \n",
    "    return precision, recall, results\n",
    "\n",
    "def evaluate_retrieval_system(test_queries, embeddings_df, top_k_values=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval system with multiple test queries and different k values.\n",
    "    \n",
    "    Args:\n",
    "        test_queries (list): List of (query_text, expected_category) tuples\n",
    "        embeddings_df (pd.DataFrame): DataFrame containing embeddings\n",
    "        top_k_values (list): Different k values to test\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for k in top_k_values:\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        print(f\"\\nEvaluating with top-{k} retrieval:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i, (query, expected_cat) in enumerate(test_queries, 1):\n",
    "            precision, recall, _ = compute_precision_recall(query, expected_cat, embeddings_df, top_k=k)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            category_name = newsgroups_train.target_names[expected_cat]\n",
    "            print(f\"Query {i}: '{query[:50]}...'\")\n",
    "            print(f\"  Category: {category_name}\")\n",
    "            print(f\"  Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "        \n",
    "        results[k] = {\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1': avg_f1,\n",
    "            'individual_precisions': precisions,\n",
    "            'individual_recalls': recalls,\n",
    "            'individual_f1s': f1_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nAverage Results for top-{k}:\")\n",
    "        print(f\"  Average Precision: {avg_precision:.3f}\")\n",
    "        print(f\"  Average Recall: {avg_recall:.3f}\")\n",
    "        print(f\"  Average F1-Score: {avg_f1:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define test queries with their expected categories\n",
    "# We'll create queries that should match specific newsgroup categories\n",
    "test_queries = [\n",
    "    (\"computer graphics and image processing\", 1),  # comp.graphics\n",
    "    (\"baseball game statistics and players\", 9),   # rec.sport.baseball\n",
    "    (\"car engine problems and repairs\", 7),        # rec.autos\n",
    "    (\"space mission and NASA news\", 15),           # sci.space\n",
    "    (\"medical health advice and symptoms\", 13),    # sci.med\n",
    "    (\"christian bible and religious discussion\", 19), # soc.religion.christian\n",
    "    (\"political government policy debate\", 17),    # talk.politics.misc\n",
    "    (\"computer hardware and electronics\", 2),      # comp.sys.mac.hardware\n",
    "    (\"motorcycle riding and maintenance\", 8),      # rec.motorcycles\n",
    "    (\"encryption and security software\", 4)        # comp.security\n",
    "]\n",
    "\n",
    "print(\"Test Queries and Expected Categories:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (query, cat_id) in enumerate(test_queries, 1):\n",
    "    category_name = newsgroups_train.target_names[cat_id]\n",
    "    print(f\"{i}. '{query}' -> {category_name}\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_retrieval_system(test_queries, df_subset, top_k_values=[5, 10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Extract data for plotting\n",
    "k_values = list(evaluation_results.keys())\n",
    "precisions = [evaluation_results[k]['precision'] for k in k_values]\n",
    "recalls = [evaluation_results[k]['recall'] for k in k_values]\n",
    "f1_scores = [evaluation_results[k]['f1'] for k in k_values]\n",
    "\n",
    "# Plot 1: Average Precision vs K\n",
    "ax1.plot(k_values, precisions, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Top-K Retrieved Documents')\n",
    "ax1.set_ylabel('Average Precision')\n",
    "ax1.set_title('Average Precision vs K')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Average Recall vs K\n",
    "ax2.plot(k_values, recalls, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Top-K Retrieved Documents')\n",
    "ax2.set_ylabel('Average Recall')\n",
    "ax2.set_title('Average Recall vs K')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Plot 3: Average F1-Score vs K\n",
    "ax3.plot(k_values, f1_scores, 'go-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Top-K Retrieved Documents')\n",
    "ax3.set_ylabel('Average F1-Score')\n",
    "ax3.set_title('Average F1-Score vs K')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Plot 4: Precision-Recall curve for different K values\n",
    "for k in k_values:\n",
    "    ax4.scatter(evaluation_results[k]['recall'], evaluation_results[k]['precision'], \n",
    "               s=100, label=f'Top-{k}', alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Recall')\n",
    "ax4.set_ylabel('Precision')\n",
    "ax4.set_title('Precision-Recall Points for Different K Values')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRIEVAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Top-K':<8} {'Avg Precision':<15} {'Avg Recall':<12} {'Avg F1-Score':<12}\")\n",
    "print(\"-\" * 47)\n",
    "for k in k_values:\n",
    "    precision = evaluation_results[k]['precision']\n",
    "    recall = evaluation_results[k]['recall']\n",
    "    f1 = evaluation_results[k]['f1']\n",
    "    print(f\"{k:<8} {precision:<15.3f} {recall:<12.3f} {f1:<12.3f}\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(df_subset)} documents\")\n",
    "print(f\"Number of test queries: {len(test_queries)}\")\n",
    "print(f\"Categories covered: {len(set([cat for _, cat in test_queries]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of individual queries\n",
    "def analyze_query_performance(query_text, expected_category, embeddings_df, top_k=10):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis for a specific query.\n",
    "    \"\"\"\n",
    "    print(f\"Detailed Analysis for Query: '{query_text}'\")\n",
    "    print(f\"Expected Category: {newsgroups_train.target_names[expected_category]}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    precision, recall, results = compute_precision_recall(query_text, expected_category, embeddings_df, top_k)\n",
    "    \n",
    "    print(f\"Precision: {precision:.3f} ({int(precision * top_k)}/{top_k} relevant documents retrieved)\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    \n",
    "    total_relevant = sum(embeddings_df['category'] == expected_category)\n",
    "    relevant_retrieved = sum(results['category'] == expected_category)\n",
    "    print(f\"Total relevant documents in dataset: {total_relevant}\")\n",
    "    print(f\"Relevant documents retrieved: {relevant_retrieved}\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} Retrieved Documents:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (_, row) in enumerate(results.iterrows(), 1):\n",
    "        category_name = newsgroups_train.target_names[row['category']]\n",
    "        is_relevant = \"✓\" if row['category'] == expected_category else \"✗\"\n",
    "        print(f\"{i:2d}. {is_relevant} Similarity: {row['similarity_score']:.3f} | Category: {category_name}\")\n",
    "        print(f\"    Text: {row['cleaned_text'][:100]}...\")\n",
    "        print()\n",
    "\n",
    "# Analyze a few specific queries in detail\n",
    "print(\"DETAILED QUERY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze the first few test queries\n",
    "for query_text, expected_cat in test_queries[:3]:\n",
    "    analyze_query_performance(query_text, expected_cat, df_subset, top_k=10)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-augmented-generation-stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
